{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Given that our stakeholders are seeking to understand what factors drive the price of a used car then their goal is to improve their sales and meet market demand. They have provided as a dataset of used car sales and have asked us to determine which factors make a car more or less expensive. A recent [report](https://www.statista.com/statistics/183713/value-of-us-passenger-cas-sales-and-leases-since-1990/) shows that used car sales doubles the amount of new car sales. The used car market is lucrative with high demand from consumers. Our stakeholder wants to stay ahead of the curve and understanding their customer's preferences can help lead to more sales. Used car dealerships also have to acquire cars for inventory from other entities. If our stakeholder can reasonably determine the cost of a used vehicle they can better understand their margins and when to purchase a vehicle for inventory. It is also important to understand what customers value in used cars. Knowing what drives a consumer to purchase a used vehicle can help our stakeholders avoid costly mistakes when acquiring inventory."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "vehicles_df = pd.read_csv(\"data/vehicles.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First we should get the basic facts of the data (describe, info, null checks, duplicate checks)\n",
    "vehicles_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The id and VIN column are not important for analysis so we will definitely remove them."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "vehicles_df.describe()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vehicles_df.describe(include=\"object\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\n",
    "    f\"Row count: {vehicles_df.shape[0]}, Duplicate count: {vehicles_df.shape[0] - vehicles_df.drop_duplicates().shape[0]}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "vehicles_df.isna().mean().round(2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vehicles_df.sample(5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Luckily, we do not have duplicates. However, we are missing values, specifically the condition and cylinders attributes are missing in over 40% of rows. Color is missing in 31% of rows. Size is missing in 72% of rows. Drive is missing in 31% of rows. I know that these attributes are important in determining the price of a car from prior analysis. The amount of missing data is concerning and the best course of action may be to impute them. That may mislead our model and we should be wary of that. However, dropping over 40% of the data isn't something we should do. One way to impute this data would be to find a finding an identical year and model then we would select the most seen value for that respective column; we can leverage scipy's `SimpleImputer`. This can help us deduce the missing attributes.\n",
    "\n",
    "I will create an imputed dataset and run my analysis on both of them."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The imputed vehicle dataframe is looking a lot better.\n",
    "<table>\n",
    "<tr>\n",
    "<th>column</th>\n",
    "<th>old missing value percent</th>\n",
    "<th>new missing value percent</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>condition</td>\n",
    "<td>0.41</td>\n",
    "<td>0.09</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>condition</td>\n",
    "<td>0.42</td>\n",
    "<td>0.14</td>\n",
    "</tr>\n",
    "<td>drive</td>\n",
    "<td>0.31</td>\n",
    "<td>0.10</td>\n",
    "</tr>\n",
    "</tr>\n",
    "<td>size</td>\n",
    "<td>0.72</td>\n",
    "<td>0.30</td>\n",
    "</tr>\n",
    "</tr>\n",
    "<td>type</td>\n",
    "<td>0.22</td>\n",
    "<td>0.05</td>\n",
    "</tr>\n",
    "</tr>\n",
    "<td>paint_color</td>\n",
    "<td>0.31</td>\n",
    "<td>0.08</td>\n",
    "</tr>\n",
    "</table?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start by checking a histogram of sales prices"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(\n",
    "    vehicles_df,\n",
    "    x=\"price\",\n",
    "    title=\"Price attribute has unrealistic values\",\n",
    "    labels={\"price\": \"Price\", \"count\": \"Count\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/box_amount_with_outliers.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(\n",
    "    vehicles_df,\n",
    "    x=\"year\",\n",
    "    labels={\"year\": \"Year\", \"count\": \"Count\"},\n",
    "    title=\"Most cars sold are 2008 or older models\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/year_box_with_outliers.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# predefine to avoid extra computation\n",
    "groupby_model_year = vehicles_df.groupby([\"model\", \"year\"])\n",
    "fields_to_impute = [\"condition\", \"cylinders\", \"drive\", \"size\", \"type\", \"paint_color\"]\n",
    "\n",
    "\n",
    "# This somewhat mimics SimpleImputer. If we wanted to get fancy we would use KNN and leverage sales price in imputation\n",
    "def impute_field_by_model_year(row, field_to_update):\n",
    "    # ensure fields are not na and some non na value exists\n",
    "    try:\n",
    "        # take the most common occurrences for model and year\n",
    "        group = groupby_model_year.get_group((row[\"model\"], row[\"year\"]))\n",
    "        non_na_values = group[field_to_update].dropna()\n",
    "        if not non_na_values.empty:\n",
    "            return non_na_values.mode().iloc[0]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# Impute once since it can be expensive\n",
    "def load_imputed_data():\n",
    "    if not os.path.exists(\"data/imputed_vehicles.csv\"):\n",
    "        # copy vehicles df and impute columns missing values\n",
    "        imputed_vehicle_df = vehicles_df.copy(deep=True)\n",
    "        for field in fields_to_impute:\n",
    "            na_filter = imputed_vehicle_df[field].isna()\n",
    "            imputed_vehicle_df.loc[na_filter, field] = imputed_vehicle_df[\n",
    "                na_filter\n",
    "            ].apply(lambda row: impute_field_by_model_year(row, field), axis=1)\n",
    "\n",
    "        imputed_vehicle_df.to_csv(\"data/imputed_vehicles.csv\")\n",
    "    else:\n",
    "        imputed_vehicle_df = pd.read_csv(\"data/imputed_vehicles.csv\")\n",
    "    return imputed_vehicle_df\n",
    "\n",
    "\n",
    "imputed_vehicle_df = load_imputed_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "imputed_vehicle_df.isna().mean().round(2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above is not realistic. We are seeing vehicles selling for billions. I will remove outliers using iqr. We also see very old cars (likely vintage) in the outlier group for year. I will remove those as well."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "q1 = vehicles_df[\"price\"].quantile(0.25)\n",
    "q3 = vehicles_df[\"price\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5 * iqr)\n",
    "upper_bound = q3 + (1.5 * iqr)\n",
    "\n",
    "vehicles_df_no_outlier = vehicles_df.query(\n",
    "    f\"price >= {lower_bound} and price <= {upper_bound} and year > 1995\"\n",
    ")\n",
    "imputed_vehicle_df = imputed_vehicle_df.query(\n",
    "    f\"price >= {lower_bound} and price <= {upper_bound} and year > 1995\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now we can drop na\n",
    "vehicles_df_no_outlier.dropna(inplace=True)\n",
    "imputed_vehicle_df.dropna(inplace=True)\n",
    "print(vehicles_df_no_outlier.shape[0])\n",
    "print(imputed_vehicle_df.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After removing outliers & dropping rows with NAs the non imputed dataset contains 33,728 rows and the imputed set contains 148,360. This is out of the original 426,880 rows."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(\n",
    "    vehicles_df_no_outlier,\n",
    "    x=\"price\",\n",
    "    title=\"Majority of vehicles fall between &#36;6,500 - &#36;21,990\",\n",
    "    labels={\"price\": \"Price\", \"count\": \"Count\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/box_amount.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    vehicles_df_no_outlier,\n",
    "    x=\"price\",\n",
    "    nbins=10,\n",
    "    labels={\"price\": \"Price\", \"count\": \"Count\"},\n",
    "    title=\"A majority of vehicles are bought at a lower price\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/price_histogram.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    vehicles_df_no_outlier,\n",
    "    x=\"year\",\n",
    "    nbins=10,\n",
    "    labels={\"year\": \"Year\", \"count\": \"Count\"},\n",
    "    title=\"Most cars sold are newer\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/year_histogram.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Univariate analysis with the imputed columns. Compare between non imputed vs imputed\n",
    "fields_to_impute = [\"condition\", \"cylinders\", \"drive\", \"size\", \"type\", \"paint_color\"]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=len(fields_to_impute),\n",
    "    cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Condition: Good and excellent were sold the most\",\n",
    "        \"Condition: Imputation increased the amount of excellent vehicles, should be cautious\",\n",
    "        \"Cylinders: 6, 4 & 8 cylinders sold the most\",\n",
    "        \"Cylinders: Imputation follows the ratio\",\n",
    "        \"Drive: 4 wheel drive is the most popular\",\n",
    "        \"Drive: Imputation follows the ratio\",\n",
    "        \"Color: White, black and silver make the majority of the sales\",\n",
    "        \"Color: Imputation follows the ratio\",\n",
    "        \"Size: Full size is the most popular\",\n",
    "        \"Size: Imputation follows the ratio\",\n",
    "        \"Type: Sedan, SUVs and pickups make the majority of the sales\",\n",
    "        \"Type: Imputation shuffled coupes, hatchbacks, convertibles and mini-vans\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i, field in enumerate(sorted(fields_to_impute)):\n",
    "    no_outlier_counts = vehicles_df_no_outlier[field].value_counts()\n",
    "    bar_vehicle_fig = go.Bar(x=no_outlier_counts.index, y=no_outlier_counts)\n",
    "    fig.add_trace(bar_vehicle_fig, row=i + 1, col=1)\n",
    "\n",
    "    imputed_counts = imputed_vehicle_df[field].value_counts()\n",
    "    bar_imputed_fig = go.Bar(x=imputed_counts.index, y=imputed_counts)\n",
    "    fig.add_trace(bar_imputed_fig, row=i + 1, col=2)\n",
    "\n",
    "fig.update_layout(height=1400, width=1400, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/imputed_fields_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imputation had some effect on our data. In particular, condition and type shuffled in terms of sales count. This is important because condition is so instrumental in determining sales price and the imputation may affect our model. We saw that type saw impact in the types with lower sales and that may not affect our model. We will create models for both datasets and use metrics to determine if our imputation helped or hurt our cause."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Univariate analysis with the non-imputed columns\n",
    "non_altered_columns = list(\n",
    "    filter(\n",
    "        lambda field: field not in fields_to_impute and field != \"VIN\",\n",
    "        list(vehicles_df_no_outlier.select_dtypes(include=\"object\").columns),\n",
    "    )\n",
    ")\n",
    "fig = make_subplots(\n",
    "    rows=len(non_altered_columns),\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Fuel: Gas is the most popular type\",\n",
    "        \"Manufacturer: Ford, Chevy and Toyota are the most popular manufacturer\",\n",
    "        \"Model: Preference is scattered. F150 tops. We see normalization of model names was not done (see silverado)\",\n",
    "        \"Region: Should represent this differently\",\n",
    "        \"State: Most populous states have most sales as expected. Should represent this differently\",\n",
    "        \"Title status: Clean title is preferred\",\n",
    "        \"Transmission: Automatic is the most popular\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i, field in enumerate(sorted(non_altered_columns)):\n",
    "    no_outlier_counts = vehicles_df_no_outlier[field].value_counts()\n",
    "    # if a value represent less than 1% of the rows we will put it in the other column\n",
    "    total_counts = no_outlier_counts.sum()\n",
    "    percent_threshold = 0.01  # 1%\n",
    "    other_mask = (no_outlier_counts / total_counts) < percent_threshold\n",
    "    other_sum = no_outlier_counts[other_mask].sum()\n",
    "    no_outlier_counts_filtered = no_outlier_counts[~other_mask]\n",
    "    if \"other\" in list(no_outlier_counts_filtered.index):\n",
    "        no_outlier_counts_filtered[\"other\"] = (\n",
    "            other_sum + no_outlier_counts_filtered[\"other\"]\n",
    "        )\n",
    "    else:\n",
    "        no_outlier_counts_filtered[\"other\"] = other_sum\n",
    "    bar_vehicle_fig = go.Bar(\n",
    "        x=no_outlier_counts_filtered.index, y=no_outlier_counts_filtered\n",
    "    )\n",
    "    fig.add_trace(bar_vehicle_fig, row=i + 1, col=1)\n",
    "\n",
    "fig.update_layout(height=1400, width=1400, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/univariate_non_imputed_fields_bar.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above analysis shows that the region, model and state should be represented differently.\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.treemap(\n",
    "    vehicles_df_no_outlier,\n",
    "    path=[px.Constant(\"USA\"), \"state\", \"region\"],\n",
    "    values=\"price\",\n",
    "    color=\"price\",\n",
    "    hover_data=[\"region\"],\n",
    "    color_continuous_scale=\"Magma\",\n",
    "    color_continuous_midpoint=np.average(vehicles_df_no_outlier[\"price\"]),\n",
    "    title=\"California has highest volume of sales and North Carolina has highest average\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=2000, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/treemap_no_outlier.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.treemap(\n",
    "    imputed_vehicle_df,\n",
    "    path=[px.Constant(\"USA\"), \"state\", \"region\"],\n",
    "    values=\"price\",\n",
    "    color=\"price\",\n",
    "    hover_data=[\"region\"],\n",
    "    color_continuous_scale=\"Magma\",\n",
    "    color_continuous_midpoint=np.average(imputed_vehicle_df[\"price\"]),\n",
    "    title=\"California retains highest volume of sales but now Washington has highest average\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=2000, showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"images/treemap_imputated.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the treemaps we see that the most populated states have the highest volume. However, other states have a higher price average. This could be indicative of different needs in different regions."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.treemap(\n",
    "    vehicles_df_no_outlier,\n",
    "    path=[\"manufacturer\", \"model\"],\n",
    "    values=\"price\",\n",
    "    color=\"price\",\n",
    "    hover_data=[\"region\"],\n",
    "    color_continuous_scale=\"Magma\",\n",
    "    color_continuous_midpoint=np.average(vehicles_df_no_outlier[\"price\"]),\n",
    "    title=\"Model does vary by price but the high cardinality may be bad for a model\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=2000, showlegend=False)\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"images/treemap_model_no_outlier.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_price_by_manufacturer = (\n",
    "    vehicles_df_no_outlier.groupby(\"manufacturer\")[\"price\"]\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    avg_price_by_manufacturer,\n",
    "    x=\"manufacturer\",\n",
    "    y=\"price\",\n",
    "    title=\"Ford is the most popular yet has a higher average price. Ferrari is missing data\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=1000)\n",
    "fig.show()\n",
    "fig.write_image(\"images/avg_price_by_manufacturer_no_outlier.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_price_by_manufacturer = (\n",
    "    imputed_vehicle_df.groupby(\"manufacturer\")[\"price\"]\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "fig = px.bar(\n",
    "    avg_price_by_manufacturer,\n",
    "    x=\"manufacturer\",\n",
    "    y=\"price\",\n",
    "    title=\"With imputation Telsa takes the highest price\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/avg_price_by_manufacturer_imputed.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I have seen enough to conclude that our imputed data set is good enough to work with moving forward. I will still build models for each to determine the which is best."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's do an average price by feature bar char\n",
    "category_columns_to_ignore = [\n",
    "    \"VIN\",\n",
    "    \"region\",\n",
    "    \"model\",\n",
    "    \"manufacturer\",\n",
    "    \"state\",\n",
    "    \"transmission\",\n",
    "]\n",
    "category_columns_graph = list(\n",
    "    filter(\n",
    "        lambda field: field not in category_columns_to_ignore,\n",
    "        list(vehicles_df_no_outlier.select_dtypes(include=\"object\").columns),\n",
    "    )\n",
    ")\n",
    "fig = make_subplots(\n",
    "    rows=len(category_columns_graph),\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Condition: New has the highest average price, typically better condition equals higher average price\",\n",
    "        \"Cylinders: More cylinders means more power means higher average price\",\n",
    "        \"Drive: 4 wheel drive is more expensive on average\",\n",
    "        \"Fuel: Diesel is the most expensive on average\",\n",
    "        \"Color: Has an affect on average price\",\n",
    "        \"Size: Full size vehicles are more expensive on average\",\n",
    "        \"Title Status: Vehicles with liens are more expensive, likely because the lien needs to be paid off\",\n",
    "        \"Type: Trucks, pickups and offroad vehicles are more expensive on average\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for i, field in enumerate(sorted(category_columns_graph)):\n",
    "    no_outlier_counts = imputed_vehicle_df[field].value_counts()\n",
    "    avg_price_by_field = (\n",
    "        imputed_vehicle_df.groupby(field)[\"price\"].mean().sort_values().reset_index()\n",
    "    )\n",
    "    bar_fig = go.Bar(x=avg_price_by_field[field], y=avg_price_by_field[\"price\"])\n",
    "    fig.add_trace(bar_fig, row=i + 1, col=1)\n",
    "\n",
    "fig.update_layout(height=1400, width=1400, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/average_price_by_field.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation plots\n",
    "corr_matrix = (\n",
    "    imputed_vehicle_df[[\"price\", \"year\", \"odometer\"]].corr(numeric_only=True).round(2)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    title=\"Newer cars are priced higher, mileage is negatively correlated with year and price\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=1000, showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/correlation.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This correlation gave me the idea to add the feature below. Average miles per year."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_year = 2025\n",
    "vehicles_df_no_outlier[\"age\"] = current_year - vehicles_df_no_outlier[\"year\"]\n",
    "imputed_vehicle_df[\"age\"] = current_year - imputed_vehicle_df[\"year\"]\n",
    "vehicles_df_no_outlier[\"avg_miles_per_year\"] = (\n",
    "    vehicles_df_no_outlier[\"odometer\"] / vehicles_df_no_outlier[\"age\"]\n",
    ")\n",
    "imputed_vehicle_df[\"avg_miles_per_year\"] = (\n",
    "    vehicles_df_no_outlier[\"odometer\"] / vehicles_df_no_outlier[\"age\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.histogram(\n",
    "    imputed_vehicle_df,\n",
    "    x=\"avg_miles_per_year\",\n",
    "    labels={\"avg_miles_per_year\": \"Avg miles per year\", \"count\": \"Count\"},\n",
    "    title=\"Average miles per year follows a normal distribution but we have outliers\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/avg_miles_per_year_histogram.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.box(\n",
    "    imputed_vehicle_df,\n",
    "    x=\"avg_miles_per_year\",\n",
    "    labels={\"avg_miles_per_year\": \"Avg miles per year\"},\n",
    "    title=\"Most vehicles are adding 5k-10k miles a year\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/avg_miles_per_year_box.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = px.bar(\n",
    "    imputed_vehicle_df.groupby(\"age\")[\"price\"].mean().sort_values().reset_index(),\n",
    "    y=\"price\",\n",
    "    x=\"age\",\n",
    "    title=\"Price decreases as age increases\",\n",
    "    labels={\"price\": \"Price\", \"age\": \"Age\"},\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/average_price_by_age.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation plots\n",
    "corr_matrix = (\n",
    "    imputed_vehicle_df[[\"price\", \"year\", \"odometer\", \"avg_miles_per_year\", \"age\"]]\n",
    "    .corr(numeric_only=True)\n",
    "    .round(2)\n",
    ")\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    title=\"Avg miles per year shows no correlation with price, indicates non linear relationship\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "fig.update_layout(height=1000, width=1000)\n",
    "fig.show()\n",
    "fig.write_image(\"images/correlation_new_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vehicles_df_numeric = imputed_vehicle_df.select_dtypes(\"number\").drop(\n",
    "    [\"price\", \"Unnamed: 0\", \"id\"], axis=1\n",
    ")\n",
    "\n",
    "pca = PCA(n_components=4).fit(vehicles_df_numeric.dropna())\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig = px.line(\n",
    "    explained_variance,\n",
    "    labels={\"index\": \"Principal Component\", \"value\": \"Explained Variance Ratio\"},\n",
    "    title=\"With only two numerical features tough to use PCA\",\n",
    "    markers=True,\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(\"images/pca.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the data understanding section, I imputed data, removed outliers and added new features as part. I will repeat the process."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reload dataset\n",
    "vehicles_df = pd.read_csv(\"data/vehicles.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop columns (id, vin, region)\n",
    "vehicles_df = vehicles_df.drop([\"id\", \"VIN\", \"region\"], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove outliers\n",
    "q1 = vehicles_df[\"price\"].quantile(0.25)\n",
    "q3 = vehicles_df[\"price\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5 * iqr)\n",
    "upper_bound = q3 + (1.5 * iqr)\n",
    "vehicles_df = vehicles_df.query(\n",
    "    f\"price >= {lower_bound} and price <= {upper_bound} and year > 1995\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# impute data\n",
    "# predefine to avoid extra computation\n",
    "groupby_model_year = vehicles_df.groupby([\"model\", \"year\"])\n",
    "fields_to_impute = [\"condition\", \"cylinders\", \"drive\", \"size\", \"type\", \"paint_color\"]\n",
    "\n",
    "\n",
    "# Impute once since it can be expensive\n",
    "def load_imputed_data_data_prep():\n",
    "    for field in fields_to_impute:\n",
    "        na_filter = vehicles_df[field].isna()\n",
    "        vehicles_df.loc[na_filter, field] = vehicles_df[na_filter].apply(\n",
    "            lambda row: impute_field_by_model_year(row, field), axis=1\n",
    "        )\n",
    "\n",
    "    return vehicles_df\n",
    "\n",
    "\n",
    "vehicles_df = load_imputed_data_data_prep()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check impute change\n",
    "vehicles_df.isna().mean().round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add features\n",
    "current_year = 2025\n",
    "vehicles_df[\"age\"] = current_year - vehicles_df[\"year\"]\n",
    "vehicles_df[\"avg_miles_per_year\"] = vehicles_df[\"odometer\"] / vehicles_df[\"age\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop duplicates and rows with NA\n",
    "vehicles_df = vehicles_df.drop_duplicates()\n",
    "vehicles_df = vehicles_df.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vehicles_df.shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vehicles_df[\"title_status\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vehicles_df[\"condition\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define our columns.\n",
    "numerical_columns = [\"age\", \"odometer\", \"avg_miles_per_year\"]\n",
    "category_columns = [\n",
    "    \"manufacturer\",\n",
    "    \"cylinders\",\n",
    "    \"fuel\",\n",
    "    \"transmission\",\n",
    "    \"drive\",\n",
    "    \"size\",\n",
    "    \"type\",\n",
    "    \"paint_color\",\n",
    "    \"state\",\n",
    "]\n",
    "title_status_columns = [\"title_status\"]\n",
    "condition_columns = [\"condition\"]\n",
    "title_statuses = [\"clean\", \"lien\", \"rebuilt\", \"salvage\", \"missing\", \"parts only\"]\n",
    "condition_values = [\"new\", \"like new\", \"excellent\", \"good\", \"fair\", \"salvage\"]\n",
    "\n",
    "# added after evaluation, remove paint as a feature\n",
    "category_columns_no_paint = [\n",
    "    \"manufacturer\",\n",
    "    \"cylinders\",\n",
    "    \"fuel\",\n",
    "    \"transmission\",\n",
    "    \"drive\",\n",
    "    \"size\",\n",
    "    \"type\",\n",
    "    \"state\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"poly\", PolynomialFeatures(include_bias=False), numerical_columns),\n",
    "        (\n",
    "            \"ordinal_title\",\n",
    "            OrdinalEncoder(categories=[title_statuses]),\n",
    "            title_status_columns,\n",
    "        ),\n",
    "        (\n",
    "            \"ordinal_condition\",\n",
    "            OrdinalEncoder(categories=[condition_values]),\n",
    "            condition_columns,\n",
    "        ),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), category_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# added after evaluation, remove paint as a feature\n",
    "preprocessor_no_paint = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"poly\", PolynomialFeatures(include_bias=False), numerical_columns),\n",
    "        (\n",
    "            \"ordinal_title\",\n",
    "            OrdinalEncoder(categories=[title_statuses]),\n",
    "            title_status_columns,\n",
    "        ),\n",
    "        (\n",
    "            \"ordinal_condition\",\n",
    "            OrdinalEncoder(categories=[condition_values]),\n",
    "            condition_columns,\n",
    "        ),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), category_columns_no_paint),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def eval_model(pipe, model_name, X_train, X_test, y_train, y_test, best_params):\n",
    "    train_mae = mean_absolute_error(pipe.predict(X_train), y_train)\n",
    "    train_mse = mean_squared_error(pipe.predict(X_train), y_train)\n",
    "    train_rmse = root_mean_squared_error(pipe.predict(X_train), y_train)\n",
    "\n",
    "    test_mae = mean_absolute_error(pipe.predict(X_test), y_test)\n",
    "    test_mse = mean_squared_error(pipe.predict(X_test), y_test)\n",
    "    test_rmse = root_mean_squared_error(pipe.predict(X_test), y_test)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_mse\": train_mse,\n",
    "        \"test_mse\": test_mse,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"coef\": (\n",
    "            pipe.named_steps[model_name].coef_\n",
    "            if hasattr(pipe.named_steps[model_name], \"coef_\")\n",
    "            else None\n",
    "        ),\n",
    "        \"best_params\": best_params,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scores = []",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split data\n",
    "X = vehicles_df.drop(\"price\", axis=1)\n",
    "y = vehicles_df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LinearRegression\n",
    "\n",
    "linreg_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"LinearRegression\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4, 5, 6],\n",
    "}\n",
    "\n",
    "grid_linreg = GridSearchCV(\n",
    "    estimator=linreg_pipe, param_grid=param_grid, cv=5, n_jobs=-1\n",
    ")\n",
    "grid_linreg.fit(X_train, y_train)\n",
    "print(grid_linreg.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Added after model evaluation. Remove paint as a feature\n",
    "linreg_pipe_no_paint = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor_no_paint),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"LinearRegressionNoPaint\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4, 5, 6],\n",
    "}\n",
    "\n",
    "grid_linreg_no_paint = GridSearchCV(\n",
    "    estimator=linreg_pipe_no_paint, param_grid=param_grid, cv=5, n_jobs=-1\n",
    ")\n",
    "grid_linreg_no_paint.fit(X_train, y_train)\n",
    "print(grid_linreg_no_paint.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "linreg_evaluation = eval_model(\n",
    "    grid_linreg.best_estimator_,\n",
    "    \"LinearRegression\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_linreg.best_params_,\n",
    ")\n",
    "scores.append(linreg_evaluation)\n",
    "\n",
    "# Added after model evaluation. Remove paint as a feature\n",
    "linreg_evaluation_no_paint = eval_model(\n",
    "    grid_linreg_no_paint.best_estimator_,\n",
    "    \"LinearRegressionNoPaint\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_linreg_no_paint.best_params_,\n",
    ")\n",
    "scores.append(linreg_evaluation_no_paint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ridge\n",
    "ridge_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"Ridge\", Ridge()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4, 5, 6],\n",
    "    \"Ridge__alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "grid_ridge = GridSearchCV(estimator=ridge_pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "print(grid_ridge.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Added after model evaluation. Remove paint as feature\n",
    "ridge_pipe_no_paint = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor_no_paint),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"RidgeNoPaint\", Ridge()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4, 5, 6],\n",
    "    \"RidgeNoPaint__alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "grid_ridge_no_paint = GridSearchCV(\n",
    "    estimator=ridge_pipe_no_paint, param_grid=param_grid, cv=5, n_jobs=-1\n",
    ")\n",
    "grid_ridge_no_paint.fit(X_train, y_train)\n",
    "print(grid_ridge_no_paint.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ridge_evaluation = eval_model(\n",
    "    grid_ridge.best_estimator_,\n",
    "    \"Ridge\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_ridge.best_params_,\n",
    ")\n",
    "scores.append(ridge_evaluation)\n",
    "\n",
    "# Added after model evaluation. Remove paint as a feature\n",
    "ridge_evaluation_no_paint = eval_model(\n",
    "    grid_ridge_no_paint.best_estimator_,\n",
    "    \"RidgeNoPaint\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_ridge_no_paint.best_params_,\n",
    ")\n",
    "scores.append(ridge_evaluation_no_paint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lasso\n",
    "# Was seeing ConvergenceWarning so ignoring it above, reduce degree for speed. Saw no advantage above 3\n",
    "lasso_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"Lasso\", Lasso(max_iter=15000, tol=1e-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4],\n",
    "    \"Lasso__alpha\": [1.0, 5, 10, 50, 100],\n",
    "}\n",
    "\n",
    "grid_lasso = GridSearchCV(estimator=lasso_pipe, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "print(grid_lasso.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Added after model evaluation. Remove paint as a feature\n",
    "lasso_pipe_no_paint = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor_no_paint),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"LassoNoPaint\", Lasso(max_iter=15000, tol=1e-1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4],\n",
    "    \"LassoNoPaint__alpha\": [1.0, 5, 10, 50, 100],\n",
    "}\n",
    "\n",
    "grid_lasso_no_paint = GridSearchCV(\n",
    "    estimator=lasso_pipe_no_paint, param_grid=param_grid, cv=3, n_jobs=-1\n",
    ")\n",
    "grid_lasso_no_paint.fit(X_train, y_train)\n",
    "print(grid_lasso_no_paint.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lasso_evaluation = eval_model(\n",
    "    grid_lasso.best_estimator_,\n",
    "    \"Lasso\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_lasso.best_params_,\n",
    ")\n",
    "scores.append(lasso_evaluation)\n",
    "\n",
    "lasso_evaluation_no_paint = eval_model(\n",
    "    grid_lasso_no_paint.best_estimator_,\n",
    "    \"LassoNoPaint\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_lasso_no_paint.best_params_,\n",
    ")\n",
    "scores.append(lasso_evaluation_no_paint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RandomForest\n",
    "random_forest_pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"RandomForest\", RandomForestRegressor()),\n",
    "    ]\n",
    ")\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4],\n",
    "    \"RandomForest__max_depth\": [4, 5],\n",
    "    \"RandomForest__n_estimators\": [100, 200],\n",
    "}\n",
    "\n",
    "grid_random_forest = GridSearchCV(\n",
    "    estimator=random_forest_pipe, param_grid=param_grid, cv=3, n_jobs=-1\n",
    ")\n",
    "grid_random_forest.fit(X_train, y_train)\n",
    "print(grid_random_forest.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Added after model evaluation. Remove paint as a feature\n",
    "random_forest_pipe_no_paint = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor_no_paint),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"RandomForestNoPaint\", RandomForestRegressor()),\n",
    "    ]\n",
    ")\n",
    "param_grid = {\n",
    "    \"preprocessor__poly__degree\": [2, 3, 4],\n",
    "    \"RandomForestNoPaint__max_depth\": [4, 5],\n",
    "    \"RandomForestNoPaint__n_estimators\": [100, 200],\n",
    "}\n",
    "\n",
    "grid_random_forest_no_paint = GridSearchCV(\n",
    "    estimator=random_forest_pipe_no_paint, param_grid=param_grid, cv=3, n_jobs=-1\n",
    ")\n",
    "grid_random_forest_no_paint.fit(X_train, y_train)\n",
    "print(grid_random_forest_no_paint.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_forest_evaluation = eval_model(\n",
    "    grid_random_forest.best_estimator_,\n",
    "    \"RandomForest\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_random_forest.best_params_,\n",
    ")\n",
    "scores.append(random_forest_evaluation)\n",
    "\n",
    "random_forest_evaluation_no_paint = eval_model(\n",
    "    grid_random_forest_no_paint.best_estimator_,\n",
    "    \"RandomForestNoPaint\",\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    grid_random_forest_no_paint.best_params_,\n",
    ")\n",
    "scores.append(random_forest_evaluation_no_paint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.to_csv(\"data/scores_df.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_scores_data():\n",
    "    if os.path.exists(\"data/scores_df.csv\"):\n",
    "        return pd.read_csv(\"data/scores_df.csv\")\n",
    "    print(\"Scores csv doesn't exist, train and score models first\")\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "scores_df = load_scores_data()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "train_bar = go.Bar(\n",
    "    x=scores_df[\"model_name\"], y=scores_df[\"train_mae\"], name=\"Train MAE\"\n",
    ")\n",
    "fig.add_trace(train_bar)\n",
    "test_bar = go.Bar(x=scores_df[\"model_name\"], y=scores_df[\"test_mae\"], name=\"Test MAE\")\n",
    "fig.add_trace(test_bar)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Train vs Test MAE Comparison Small gap shows good generalization. Ridge most performant\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Mean Absolute Error (MAE)\",\n",
    "    barmode=\"group\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    showlegend=True,\n",
    "    bargap=0.5,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/mae_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "train_bar = go.Bar(\n",
    "    x=scores_df[\"model_name\"], y=scores_df[\"train_mse\"], name=\"Train MSE\"\n",
    ")\n",
    "fig.add_trace(train_bar)\n",
    "test_bar = go.Bar(x=scores_df[\"model_name\"], y=scores_df[\"test_mse\"], name=\"Test MSE\")\n",
    "fig.add_trace(test_bar)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Train vs Test MSE Comparison Small gap shows good generalization. Ridge most performant\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Mean Squared Error (MSE)\",\n",
    "    barmode=\"group\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    showlegend=True,\n",
    "    bargap=0.5,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/mse_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "train_bar = go.Bar(\n",
    "    x=scores_df[\"model_name\"], y=scores_df[\"train_rmse\"], name=\"Train RMSE\"\n",
    ")\n",
    "fig.add_trace(train_bar)\n",
    "test_bar = go.Bar(x=scores_df[\"model_name\"], y=scores_df[\"test_rmse\"], name=\"Test RMSE\")\n",
    "fig.add_trace(test_bar)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Train vs Test RMSE Comparison Small gap shows good generalization. Ridge most performant\",\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Root Mean Squared Error (RMSE)\",\n",
    "    barmode=\"group\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    showlegend=True,\n",
    "    bargap=0.5,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/rmse_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_feature_names(grid, is_paint=True):\n",
    "    preprocessor = grid.best_estimator_.named_steps[\"preprocessor\"]\n",
    "    poly_features = preprocessor.named_transformers_[\"poly\"].get_feature_names_out(numerical_columns)\n",
    "    ordinal_title_features = preprocessor.named_transformers_[\"ordinal_title\"].get_feature_names_out(title_status_columns)\n",
    "    ordinal_condition_features = preprocessor.named_transformers_[\"ordinal_condition\"].get_feature_names_out(condition_columns)\n",
    "    if is_paint:\n",
    "        onehot_features = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(category_columns)\n",
    "    else:\n",
    "        onehot_features = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(category_columns_no_paint)\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            poly_features,\n",
    "            ordinal_title_features,\n",
    "            ordinal_condition_features,\n",
    "            onehot_features,\n",
    "        ]\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get feature importance for each model\n",
    "\n",
    "linear_importance = {\n",
    "    \"Feature\": get_feature_names(grid_linreg),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_linreg.best_estimator_.named_steps[\"LinearRegression\"].coef_\n",
    "    ),\n",
    "}\n",
    "linear_importance_df = pd.DataFrame(linear_importance)\n",
    "\n",
    "ridge_importance = {\n",
    "    \"Feature\": get_feature_names(grid_ridge),\n",
    "    \"Importance - Coef\": np.abs(grid_ridge.best_estimator_.named_steps[\"Ridge\"].coef_),\n",
    "}\n",
    "ridge_importance_df = pd.DataFrame(ridge_importance)\n",
    "\n",
    "lasso_importance = {\n",
    "    \"Feature\": get_feature_names(grid_lasso),\n",
    "    \"Importance - Coef\": np.abs(grid_lasso.best_estimator_.named_steps[\"Lasso\"].coef_),\n",
    "}\n",
    "lasso_importance_df = pd.DataFrame(lasso_importance)\n",
    "\n",
    "random_forest_importance = {\n",
    "    \"Feature\": get_feature_names(grid_random_forest),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_random_forest.best_estimator_.named_steps[\n",
    "            \"RandomForest\"\n",
    "        ].feature_importances_\n",
    "    ),\n",
    "}\n",
    "random_forest_importance_df = pd.DataFrame(random_forest_importance)\n",
    "\n",
    "# Now adding estimators without paint\n",
    "\n",
    "linear_importance_no_paint = {\n",
    "    \"Feature\": get_feature_names(grid_linreg_no_paint, is_paint=False),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_linreg_no_paint.best_estimator_.named_steps[\n",
    "            \"LinearRegressionNoPaint\"\n",
    "        ].coef_\n",
    "    ),\n",
    "}\n",
    "linear_importance_no_paint_df = pd.DataFrame(linear_importance_no_paint)\n",
    "\n",
    "ridge_importance_no_paint = {\n",
    "    \"Feature\": get_feature_names(grid_ridge_no_paint, is_paint=False),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_ridge_no_paint.best_estimator_.named_steps[\"RidgeNoPaint\"].coef_\n",
    "    ),\n",
    "}\n",
    "ridge_importance_no_paint_df = pd.DataFrame(ridge_importance_no_paint)\n",
    "\n",
    "lasso_importance_no_paint = {\n",
    "    \"Feature\": get_feature_names(grid_lasso_no_paint, is_paint=False),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_lasso_no_paint.best_estimator_.named_steps[\"LassoNoPaint\"].coef_\n",
    "    ),\n",
    "}\n",
    "lasso_importance_no_paint_df = pd.DataFrame(lasso_importance_no_paint)\n",
    "\n",
    "random_forest_importance_no_paint = {\n",
    "    \"Feature\": get_feature_names(grid_random_forest_no_paint, is_paint=False),\n",
    "    \"Importance - Coef\": np.abs(\n",
    "        grid_random_forest_no_paint.best_estimator_.named_steps[\n",
    "            \"RandomForestNoPaint\"\n",
    "        ].feature_importances_\n",
    "    ),\n",
    "}\n",
    "random_forest_importance_no_paint_df = pd.DataFrame(random_forest_importance_no_paint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "linear_importance_df = linear_importance_df.sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    linear_importance_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features LinearRegression\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/linear_regression_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "linear_importance_no_paint_df = linear_importance_no_paint_df.sort_values(\n",
    "    \"Importance - Coef\", ascending=False\n",
    ")\n",
    "fig = px.bar(\n",
    "    linear_importance_no_paint_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features No Paint LinearRegression\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/linear_regression_no_paint_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ridge_importance_df = ridge_importance_df.sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    ridge_importance_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features Ridge\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/ridge_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ridge_importance_no_paint_df = ridge_importance_no_paint_df.sort_values(\n",
    "    \"Importance - Coef\", ascending=False\n",
    ")\n",
    "fig = px.bar(\n",
    "    ridge_importance_no_paint_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features No Paint Ridge\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/ridge_no_paint_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lasso_importance_df = lasso_importance_df[\n",
    "    lasso_importance_df[\"Importance - Coef\"] > 0\n",
    "].sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    lasso_importance_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features Lasso\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/lasso_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lasso_importance_no_paint_df = lasso_importance_no_paint_df[\n",
    "    lasso_importance_no_paint_df[\"Importance - Coef\"] > 0\n",
    "].sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    lasso_importance_no_paint_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features Lasso\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/lasso_no_paint_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_forest_importance_df = random_forest_importance_df[\n",
    "    random_forest_importance_df[\"Importance - Coef\"] > 0\n",
    "].sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    random_forest_importance_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features RandomForest\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/random_forest_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_forest_importance_no_paint_df = random_forest_importance_no_paint_df[\n",
    "    random_forest_importance_no_paint_df[\"Importance - Coef\"] > 0\n",
    "].sort_values(\"Importance - Coef\", ascending=False)\n",
    "fig = px.bar(\n",
    "    random_forest_importance_no_paint_df[:10],\n",
    "    x=\"Importance - Coef\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Top Ten Features No Paint RandomForest\",\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(\"images/random_forest_no_paint_top_features.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I am going to go back and make models without paint in the feature set.\n",
    "\n",
    "After testing, removing paint made our models worse! This was not what I thought would happen."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>model_name</th>\n",
    "      <th>train_mae</th>\n",
    "      <th>test_mae</th>\n",
    "      <th>train_mse</th>\n",
    "      <th>test_mse</th>\n",
    "      <th>train_rmse</th>\n",
    "      <th>test_rmse</th>\n",
    "      <th>best_params</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>LinearRegression</td>\n",
    "      <td>5283.77</td>\n",
    "      <td>5294.00</td>\n",
    "      <td>64031541.68</td>\n",
    "      <td>64194335.93</td>\n",
    "      <td>8001.97</td>\n",
    "      <td>8012.14</td>\n",
    "      <td>{'preprocessor__poly__degree': 2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>LinearRegressionNoPaint</td>\n",
    "      <td>5285.40</td>\n",
    "      <td>5296.52</td>\n",
    "      <td>64098638.01</td>\n",
    "      <td>64269098.87</td>\n",
    "      <td>8006.16</td>\n",
    "      <td>8016.80</td>\n",
    "      <td>{'preprocessor__poly__degree': 2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Ridge</td>\n",
    "      <td>5225.62</td>\n",
    "      <td>5229.38</td>\n",
    "      <td>63065606.60</td>\n",
    "      <td>63064394.53</td>\n",
    "      <td>7941.39</td>\n",
    "      <td>7941.31</td>\n",
    "      <td>{'Ridge__alpha': 0.01, 'preprocessor__poly__degree': 6}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>RidgeNoPaint</td>\n",
    "      <td>5226.53</td>\n",
    "      <td>5231.90</td>\n",
    "      <td>63124370.98</td>\n",
    "      <td>63128494.09</td>\n",
    "      <td>7945.08</td>\n",
    "      <td>7945.34</td>\n",
    "      <td>{'RidgeNoPaint__alpha': 0.01, 'preprocessor__poly__degree': 6}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Lasso</td>\n",
    "      <td>5257.35</td>\n",
    "      <td>5265.29</td>\n",
    "      <td>63636373.78</td>\n",
    "      <td>63650075.25</td>\n",
    "      <td>7977.24</td>\n",
    "      <td>7978.10</td>\n",
    "      <td>{'Lasso__alpha': 1.0, 'preprocessor__poly__degree': 3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>LassoNoPaint</td>\n",
    "      <td>5258.66</td>\n",
    "      <td>5267.62</td>\n",
    "      <td>63700182.15</td>\n",
    "      <td>63720917.01</td>\n",
    "      <td>7981.24</td>\n",
    "      <td>7982.54</td>\n",
    "      <td>{'LassoNoPaint__alpha': 1.0, 'preprocessor__poly__degree': 3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>RandomForest</td>\n",
    "      <td>5418.38</td>\n",
    "      <td>5457.99</td>\n",
    "      <td>67308779.78</td>\n",
    "      <td>67985841.22</td>\n",
    "      <td>8204.19</td>\n",
    "      <td>8245.35</td>\n",
    "      <td>{'RandomForest__max_depth': 5, 'RandomForest__n_estimators': 100, 'preprocessor__poly__degree': 4}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>RandomForestNoPaint</td>\n",
    "      <td>5417.84</td>\n",
    "      <td>5458.16</td>\n",
    "      <td>67338583.29</td>\n",
    "      <td>68029615.54</td>\n",
    "      <td>8206.01</td>\n",
    "      <td>8248.01</td>\n",
    "      <td>{'RandomForestNoPaint__max_depth': 5, 'RandomForestNoPaint__n_estimators': 200, 'preprocessor__poly__degree': 4}</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<p>\n",
    "After a thorough analysis of four different model types, the Ridge model stands out as the best. The Ridge model achieved a Mean Absolute Error of 5,229.38, a Mean Squared Error of 63,064,394.53 and a Root Square Error of 7,941.31 on the test set. The best parameters identified through cross-validation were an alpha of 0.01 and a polynomial degree of 6. Suggesting that a lower regularization effect was more effective. Interestingly, Lasso's best alpha was 1, indicating regularization was beneficial for Lasso. Removing the paint color from the models actually had a slight detrimental effect, so it would be best to keep this feature. In terms of feature importance, the top features across all models were polynomial representations of age, average miles per year and odometer readings. This indicates that our feature engineering was successful and as expected from our multivariate analysis, mileage and age significantly influence the price. Additionally, our analysis revealed that Ford, Chevy and Toyota were the most popular manufacturers. The univariate analysis showed that vehicles in excellent and good condition were most common with average prices falling in the middle. Vehicle type analysis showed interesting trends. Sedans, SUVs and trucks were the most popular but had distinct average price points. When reporting to the business we will present the Ridge model and talk about the features that were most influential and most popular. We will discus how the most important features affect the price of a vehicle.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai",
   "language": "python",
   "name": "mlai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
